# -*- coding: utf-8 -*-
"""main_MLP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SzLjRFh__5igR9zK-1A-GKBOO8HyNCam
"""

# initialize
init = tf.global_variables_initializer()

import matplotlib.pyplot as plt

# train hyperparameters
epoch_cnt = 15
batch_size = 1000
train_iteration = len(x_train) // batch_size
test_iteration = len(x_test) // batch_size

# Lists to store accuracy and loss values for plotting
train_acc_history = []
val_acc_history = []
train_loss_history = []
val_loss_history = []

# Start training
with tf.Session() as sess:
    # Run the initializer
    sess.run(init)
    for epoch in range(epoch_cnt):
        avg_train_loss = 0.
        avg_test_loss = 0.

        # Train
        for i in range(train_iteration):
            start = i * batch_size
            end = start + batch_size
            _, train_loss = sess.run([train_op, loss_op],
                                     feed_dict={x: x_train[start: end], y: y_train[start: end]})
            avg_train_loss += train_loss / train_iteration

        # Test
        for i in range(test_iteration):
            start = i * batch_size
            end = start + batch_size
            test_loss = sess.run(loss_op, feed_dict={x: x_test[start: end], y: y_test[start: end]})
            avg_test_loss += test_loss / test_iteration

        # Validate model
        preds = tf.nn.softmax(logits)  # Apply softmax to logits
        correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y, 1))
        # Calculate accuracy
        accuracy = tf.reduce_mean(tf.cast(correct_prediction, "float"))
        cur_train_acc = accuracy.eval({x: x_train, y: y_train})
        cur_val_acc = accuracy.eval({x: x_test, y: y_test})
        print("epoch: "+str(epoch)+", train accuracy: "
              + str(cur_train_acc) + ", validation accuracy: "
              + str(cur_val_acc) +', train loss: '+str(avg_train_loss) + ', test loss: '+str(avg_test_loss))

        # Save accuracy and loss for plotting
        train_acc_history.append(cur_train_acc)
        val_acc_history.append(cur_val_acc)
        train_loss_history.append(avg_train_loss)
        val_loss_history.append(avg_test_loss)

    # Plot validation accuracy and loss
    plt.figure(figsize=(15, 5))

    plt.subplot(1, 2, 1)
    plt.plot(range(epoch_cnt), train_acc_history, label='Training Accuracy', color='blue')
    plt.plot(range(epoch_cnt), val_acc_history, label='Test Accuracy', color='red')
    plt.title('MLP Training and Test Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(range(epoch_cnt), train_loss_history, label='Training Loss', color='blue')
    plt.plot(range(epoch_cnt), val_loss_history, label='Test Loss', color='red')
    plt.title('MLP Training and Test Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.tight_layout()
    plt.show()

    # Test model
    preds = tf.nn.softmax(logits)  # Apply softmax to logits
    correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y, 1))
    # Calculate accuracy
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, "float"))
    print("[Test Accuracy] :", accuracy.eval({x: x_test, y: y_test}))