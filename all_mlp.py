# -*- coding: utf-8 -*-
"""MLP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/165C6XR7rk70sJuJ9r5MEYpPbnhP-nHbB

## Collect MNIST Data
"""

# 코드에 필요한 함수 불러오기
import tensorflow as tf
import numpy as np

# MNIST 데이터 불러오기
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()

# 28개의 행렬이 있는 train와 test 데이터의 개수 출력
print(x_train.shape)
print(x_test.shape)

print("train data has " + str(x_train.shape[0]) + " samples")
print("every train data is " + str(x_train.shape[1])
      + " * " + str(x_train.shape[2]) + " image")

# sample to show gray scale values
print(x_train[0][8])

# sample to show labels for first train data to 10th train data
print(y_train[0:9])

print("test data has " + str(x_test.shape[0]) + " samples")
print("every test data is " + str(x_test.shape[1])
      + " * " + str(x_test.shape[2]) + " image")

"""## Reshape"""

# 행렬을 array로 변환
x_train = x_train.reshape(60000, 784)
x_test = x_test.reshape(10000, 784)

print(x_train.shape)
print(x_test.shape)

x_train[0]

"""## Normalize data"""

x_train = x_train.astype('float32')
x_test = x_test.astype('float32')

gray_scale = 255
x_train /= gray_scale
x_test /= gray_scale

"""## label to one hot encoding value"""

num_classes = 10
y_train = tf.keras.utils.to_categorical(y_train, num_classes)
y_test = tf.keras.utils.to_categorical(y_test, num_classes)

y_train

"""## Tensorflow MLP Graph"""

import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()

x = tf.placeholder(shape=[None, 2], dtype=tf.float32)

x = tf.placeholder(tf.float32, [None, 784])
y = tf.placeholder(tf.float32, [None, 10])

def mlp(x):
    # hidden layer1
    w1 = tf.Variable(tf.random_uniform([784,256]))
    b1 = tf.Variable(tf.zeros([256]))
    h1 = tf.nn.relu(tf.matmul(x, w1) + b1)
    # hidden layer2
    w2 = tf.Variable(tf.random_uniform([256,128]))
    b2 = tf.Variable(tf.zeros([128]))
    h2 = tf.nn.relu(tf.matmul(h1, w2) + b2)
    # output layer
    w3 = tf.Variable(tf.random_uniform([128,10]))
    b3 = tf.Variable(tf.zeros([10]))
    logits= tf.matmul(h2, w3) + b3

    return logits

logits = mlp(x)
loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(
    logits=logits, labels=y))
train_op = tf.train.AdamOptimizer(learning_rate=0.01).minimize(loss_op)

# initialize
init = tf.global_variables_initializer()

import matplotlib.pyplot as plt

# train hyperparameters
epoch_cnt = 15
batch_size = 1000
train_iteration = len(x_train) // batch_size
test_iteration = len(x_test) // batch_size

# Lists to store accuracy and loss values for plotting
train_acc_history = []
val_acc_history = []
train_loss_history = []
val_loss_history = []

# Start training
with tf.Session() as sess:
    # Run the initializer
    sess.run(init)
    for epoch in range(epoch_cnt):
        avg_train_loss = 0.
        avg_test_loss = 0.

        # Train
        for i in range(train_iteration):
            start = i * batch_size
            end = start + batch_size
            _, train_loss = sess.run([train_op, loss_op],
                                     feed_dict={x: x_train[start: end], y: y_train[start: end]})
            avg_train_loss += train_loss / train_iteration

        # Test
        for i in range(test_iteration):
            start = i * batch_size
            end = start + batch_size
            test_loss = sess.run(loss_op, feed_dict={x: x_test[start: end], y: y_test[start: end]})
            avg_test_loss += test_loss / test_iteration

        # Validate model
        preds = tf.nn.softmax(logits)  # Apply softmax to logits
        correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y, 1))
        # Calculate accuracy
        accuracy = tf.reduce_mean(tf.cast(correct_prediction, "float"))
        cur_train_acc = accuracy.eval({x: x_train, y: y_train})
        cur_val_acc = accuracy.eval({x: x_test, y: y_test})
        print("epoch: "+str(epoch)+", train accuracy: "
              + str(cur_train_acc) + ", validation accuracy: "
              + str(cur_val_acc) +', train loss: '+str(avg_train_loss) + ', test loss: '+str(avg_test_loss))

        # Save accuracy and loss for plotting
        train_acc_history.append(cur_train_acc)
        val_acc_history.append(cur_val_acc)
        train_loss_history.append(avg_train_loss)
        val_loss_history.append(avg_test_loss)

    # Plot validation accuracy and loss
    plt.figure(figsize=(15, 5))

    plt.subplot(1, 2, 1)
    plt.plot(range(epoch_cnt), train_acc_history, label='Training Accuracy', color='blue')
    plt.plot(range(epoch_cnt), val_acc_history, label='Test Accuracy', color='red')
    plt.title('MLP Training and Test Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(range(epoch_cnt), train_loss_history, label='Training Loss', color='blue')
    plt.plot(range(epoch_cnt), val_loss_history, label='Test Loss', color='red')
    plt.title('MLP Training and Test Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.tight_layout()
    plt.show()

    # Test model
    preds = tf.nn.softmax(logits)  # Apply softmax to logits
    correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y, 1))
    # Calculate accuracy
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, "float"))
    print("[Test Accuracy] :", accuracy.eval({x: x_test, y: y_test}))
